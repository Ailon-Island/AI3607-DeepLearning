{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import jittor as jt\n",
    "from jittor import transform\n",
    "\n",
    "from options import Options\n",
    "from dataset import CIFAR10Perm\n",
    "from models import Net\n",
    "\n",
    "jt.flags.use_cuda = 1\n",
    "jt.set_global_seed(0)\n",
    "\n",
    "opt = Options().parse()\n",
    "\n",
    "flip_prob = 1. - np.sqrt(1. - opt.flip_rate)\n",
    "augment = transform.Compose([\n",
    "    transform.RandomHorizontalFlip(p=flip_prob),\n",
    "    transform.RandomVerticalFlip(p=flip_prob),\n",
    "    transform.RandomResizedCrop(size=32, scale=(0.8, 1.0), ratio=(0.75, 1.33)),\n",
    "]) if opt.data_augment else None\n",
    "transform = transform.Compose([\n",
    "    transform.ToTensor(),\n",
    "    transform.ImageNormalize(mean=[0.5], std=[0.5]),\n",
    "])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, opt, total_iters, model, optimizer, loader, test_loader):\n",
    "        self.opt = opt\n",
    "        self.total_iters = total_iters\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.grad_clip = opt.use_gradient_clip\n",
    "        self.loader = loader\n",
    "        self.test_loader = test_loader\n",
    "        self.losses = {'train': [], 'test': []}\n",
    "        self.accs = {'train': [], 'test': []}\n",
    "        self.iter_footprint = {'train': [], 'test': []}\n",
    "        self.log_file = os.path.join(opt.checkpoint, opt.name, 'log.txt')\n",
    "\n",
    "    def train(self, epoch_idx):\n",
    "        self.model.train()\n",
    "\n",
    "        num_iter = 0\n",
    "        for batch_idx, (data, label) in enumerate(self.loader, start=1):\n",
    "            self.total_iters += data.shape[0]\n",
    "            num_iter += data.shape[0]\n",
    "            pred, loss = self.model(data, label)\n",
    "            self.optimizer.step(loss)\n",
    "            acc = (pred.argmax(dim=2)[0] == label).float().mean()\n",
    "\n",
    "            self.losses['train'] += [loss.item()]\n",
    "            self.accs['train'] += [acc.item()]\n",
    "            self.iter_footprint['train'] += [self.total_iters]\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                self.log('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAccuracy: {:.6f}'.format(\n",
    "                    epoch_idx, num_iter, len(self.loader),\n",
    "                    100. * num_iter / len(self.loader), loss.item(), acc.item()\n",
    "                ))\n",
    "\n",
    "            # learning rate decay\n",
    "            if opt.lr_decay_iter >= 0:\n",
    "                if self.total_iters >= opt.lr_decay_iter:\n",
    "                    if (self.total_iters - opt.lr_decay_iter) % opt.lr_decay_freq == 0:\n",
    "                        opt.lr = opt.lr * opt.lr_decay\n",
    "                        for param_group in self.optimizer.param_groups:\n",
    "                            param_group['lr'] = opt.lr\n",
    "                        self.log('Learning rate decay to {}'.format(opt.lr))\n",
    "\n",
    "    def test(self, epoch_idx):\n",
    "        self.model.eval()\n",
    "\n",
    "        test_loss, test_acc = [], []\n",
    "        total_loss, total_acc = 0., 0.\n",
    "        num_iter = 0\n",
    "        with jt.no_grad():\n",
    "            for batch_idx, (data, label) in enumerate(self.test_loader, start=1):\n",
    "                num_iter += data.shape[0]\n",
    "                pred, loss = self.model(data, label)\n",
    "                test_loss += [loss.item()]\n",
    "                acc = (pred.argmax(dim=2)[0] == label).float().mean()\n",
    "                test_acc += [acc.item()]\n",
    "                total_loss += loss.item() * data.shape[0]\n",
    "                total_acc += acc.item() * data.shape[0]\n",
    "\n",
    "                if batch_idx % 10 == 0:\n",
    "                    self.log('Testing Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAccuracy: {:.6f}'.format(\n",
    "                        epoch_idx, num_iter, len(self.test_loader),\n",
    "                        100. * num_iter / len(self.test_loader), loss.item(), acc.item()))\n",
    "\n",
    "        total_loss /= num_iter\n",
    "        total_acc /= num_iter\n",
    "        self.losses['test'] += [total_loss]\n",
    "        self.accs['test'] += [total_acc]\n",
    "        self.iter_footprint['test'] += [self.total_iters]\n",
    "\n",
    "        self.log('Test Epoch: {}, Average loss: {:.6f}, Accuracy: {:.6f}'.format(epoch_idx, total_loss, total_acc))\n",
    "\n",
    "    def save(self):\n",
    "        model_dir = os.path.join(opt.checkpoint, opt.name, 'models')\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.mkdir(model_dir)\n",
    "        model_dir = os.path.join(model_dir)\n",
    "        self.model.save(os.path.join(model_dir, 'iter_{}.pkl'.format(self.total_iters)))\n",
    "        self.model.save(os.path.join(model_dir, 'latest.pkl'))\n",
    "        self.log('Model saved to {}'.format(os.path.join(model_dir, 'iter_{}.pkl'.format(self.total_iters))))\n",
    "\n",
    "    def plot(self, save=False):\n",
    "        results_dir = os.path.join(opt.checkpoint, opt.name, 'results')\n",
    "        if not os.path.exists(results_dir):\n",
    "            os.mkdir(results_dir)\n",
    "\n",
    "        train_losses, test_losses = self.losses['train'], self.losses['test']\n",
    "        train_accs, test_accs = self.accs['train'], self.accs['test']\n",
    "\n",
    "        iters = self.iter_footprint['train']\n",
    "        epochs = self.iter_footprint['test']\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(iters, train_losses, label='Train loss')\n",
    "        plt.plot(epochs, test_losses, label='Test loss')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        if save:\n",
    "            plt.savefig(os.path.join(results_dir, 'loss.png'))\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(iters, train_accs, label='Train accuracy')\n",
    "        plt.plot(epochs, test_accs, label='Test accuracy')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        if save:\n",
    "            plt.savefig(os.path.join(results_dir, 'accuracy.png'))\n",
    "        plt.show()\n",
    "\n",
    "    def log(self, msg):\n",
    "        with open(self.log_file, 'a') as f:\n",
    "            f.write(msg+'\\n')\n",
    "            print(msg)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get Data Loaders\n",
    "print('Preparing data loaders...')\n",
    "train_loader = CIFAR10Perm(opt=opt, train=True, shuffle=True, augment=augment, transform=transform)\n",
    "test_loader = CIFAR10Perm(opt=opt, train=False, shuffle=False, augment=None, transform=transform)\n",
    "# Load the model\n",
    "print('Loading the model...')\n",
    "model = Net(opt)\n",
    "total_iters = 0\n",
    "optimizer = jt.optim.Adam(model.parameters(), lr=opt.lr, weight_decay=opt.weight_decay)\n",
    "\n",
    "trainer = Trainer(opt, total_iters, model, optimizer, train_loader, test_loader)\n",
    "\n",
    "# Train & test the model\n",
    "print('Preparation done. Now start training.')\n",
    "for epoch_idx in range(1, opt.max_epoch+1):\n",
    "    epoch_start_time = time.time()\n",
    "    print('Epoch: {}'.format(epoch_idx))\n",
    "\n",
    "    trainer.train(epoch_idx)\n",
    "    trainer.test(epoch_idx)\n",
    "\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "    # visualization\n",
    "    print(f'End of epoch: {epoch_idx}\\ttime: {epoch_time:.2f}s')\n",
    "    trainer.plot(save=True)\n",
    "\n",
    "    # save model\n",
    "    if epoch_idx % opt.save_freq == 0:\n",
    "        trainer.save()\n",
    "\n",
    "    if trainer.total_iters >= opt.max_iter:\n",
    "        print('Reach max iteration. Stop training.')\n",
    "        break\n",
    "\n",
    "# Save the results\n",
    "trainer.plot(save=True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
